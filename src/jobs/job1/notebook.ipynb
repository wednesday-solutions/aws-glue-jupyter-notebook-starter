{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import sys\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.sql import functions as F\n",
    "from utils.env import load_env, get_env_or_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELATIVE_PATH_PREFIX=\"./../../..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_write_data(data_frame, target_bucket, target_directory, environment):\n",
    "    # Converting all column names to lower snake_case\n",
    "    for col_name in data_frame.columns:\n",
    "        snake_case_name = col_name.lower().replace(\" \", \"_\")\n",
    "        data_frame = data_frame.withColumnRenamed(col_name, snake_case_name)\n",
    "\n",
    "    # Computing full_name by concatenating first_name and last_name\n",
    "    data_frame = data_frame.withColumn(\"full_name\", F.concat_ws(\" \", data_frame[\"first_name\"], data_frame[\"last_name\"]))\n",
    "\n",
    "    # Determine the output path based on the environment\n",
    "    if environment == 'local':\n",
    "\n",
    "        output_path = os.path.join(target_directory, 'output.csv')\n",
    "        print(output_path)\n",
    "        data_frame.write.csv(output_path, mode='overwrite', header=True)\n",
    "    else:\n",
    "        output_path = f\"s3://{target_bucket}/{target_directory}\"\n",
    "        data_frame.write.csv(output_path, mode='overwrite', header=True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(environment, local_file_path, crawler_name):\n",
    "    if environment == \"local\":\n",
    "        # Read data from local file (e.g., using Pandas)\n",
    "        data = spark.read.csv(local_file_path, header=True)\n",
    "    else:\n",
    "        # Create or retrieve a Spark context\n",
    "        sc = SparkContext.getOrCreate()\n",
    "        glueContext = GlueContext(sc)\n",
    "        glue_client = boto3.client('glue')\n",
    "\n",
    "        # Get the crawler metadata\n",
    "        crawler_metadata = glue_client.get_crawler(Name=crawler_name)\n",
    "\n",
    "        # keys = list(crawler_metadata['Crawler'].keys())\n",
    "\n",
    "        database_name = crawler_metadata['Crawler']['DatabaseName']\n",
    "        prefix = crawler_metadata['Crawler']['Targets']['S3Targets'][0]['Path'] if 'S3Targets' in crawler_metadata['Crawler']['Targets'] else None\n",
    "\n",
    "        # Get table names from the Glue Data Catalog using the database and prefix\n",
    "        response = glue_client.get_tables(DatabaseName=database_name)\n",
    "        tables = [table['Name'] for table in response['TableList']]\n",
    "        table_name = tables[0]\n",
    "        # Read data from Glue Data Catalog\n",
    "        dynamic_frame = glueContext.create_dynamic_frame.from_catalog(\n",
    "            database=database_name,\n",
    "            table_name=table_name\n",
    "        )\n",
    "\n",
    "        # Convert to a Spark DataFrame (or Pandas DataFrame if needed)\n",
    "        data = dynamic_frame.toDF()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = get_env_or_args(\"ENVIRONMENT_NAME\")\n",
    "load_env(environment)\n",
    "local_file_path = f\"{RELATIVE_PATH_PREFIX}/data/raw/sample.csv\"\n",
    "crawler_name = get_env_or_args(\"SOURCE_CRAWLER\")\n",
    "data = read_data(get_env_or_args(\"ENVIRONMENT_NAME\"), local_file_path, crawler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+-------------+\n",
      "|First Name|Last Name|               Email|Year of Birth|\n",
      "+----------+---------+--------------------+-------------+\n",
      "|     Rohan|    Gupta|rohan.gupta@wedne...|         1992|\n",
      "+----------+---------+--------------------+-------------+\n",
      "\n",
      "None\n",
      "./../../../landing/job1/output.csv\n",
      "+----------+---------+--------------------+-------------+-----------+\n",
      "|first_name|last_name|               email|year_of_birth|  full_name|\n",
      "+----------+---------+--------------------+-------------+-----------+\n",
      "|     Rohan|    Gupta|rohan.gupta@wedne...|         1992|Rohan Gupta|\n",
      "+----------+---------+--------------------+-------------+-----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.show())\n",
    "\n",
    "target_bucket = get_env_or_args(\"BUCKET_NAME\")\n",
    "\n",
    "target_directory = f\"{RELATIVE_PATH_PREFIX if environment == 'local' else target_bucket}/landing/{get_env_or_args('JOB_NAME')}\"\n",
    "data = transform_and_write_data(data, target_bucket, target_directory, environment)\n",
    "\n",
    "print(data.show())\n",
    "job.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
