{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import sys\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, current_date, year\n",
    "from utils.env import load_env, get_env_or_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELATIVE_PATH_PREFIX=\"./../../..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_age(data_frame):\n",
    "    # Calculating the current year using the current date\n",
    "    current_year = year(current_date())\n",
    "\n",
    "    # Calculating the age by subtracting the year_of_birth from the current year\n",
    "    age_column = current_year - col(\"year_of_birth\")\n",
    "\n",
    "    # Adding the calculated age as a new column to the DataFrame\n",
    "    data_frame_with_age = data_frame.withColumn(\"age\", age_column)\n",
    "\n",
    "    return data_frame_with_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_lower_case(data_frame):\n",
    "    # Converting all column names to lower snake_case\n",
    "    for col_name in data_frame.columns:\n",
    "        snake_case_name = col_name.lower().replace(\" \", \"_\")\n",
    "        data_frame = data_frame.withColumnRenamed(col_name, snake_case_name)\n",
    "\n",
    "    # Computing full_name by concatenating first_name and last_name\n",
    "    data_frame = data_frame.withColumn(\"full_name\", F.concat_ws(\" \", data_frame[\"first_name\"], data_frame[\"last_name\"]))\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(environment, local_file_path, crawler_name):\n",
    "    if environment == \"local\":\n",
    "        # Read data from local file (e.g., using Pandas)\n",
    "        data = spark.read.csv(local_file_path, header=True)\n",
    "    else:\n",
    "        # Create or retrieve a Spark context\n",
    "        sc = SparkContext.getOrCreate()\n",
    "        glueContext = GlueContext(sc)\n",
    "        glue_client = boto3.client('glue')\n",
    "\n",
    "        # Get the crawler metadata\n",
    "        crawler_metadata = glue_client.get_crawler(Name=crawler_name)\n",
    "\n",
    "        # keys = list(crawler_metadata['Crawler'].keys())\n",
    "\n",
    "        database_name = crawler_metadata['Crawler']['DatabaseName']\n",
    "        prefix = crawler_metadata['Crawler']['Targets']['S3Targets'][0]['Path'] if 'S3Targets' in crawler_metadata['Crawler']['Targets'] else None\n",
    "\n",
    "        # Get table names from the Glue Data Catalog using the database and prefix\n",
    "        response = glue_client.get_tables(DatabaseName=database_name)\n",
    "        # error_message = f\"The keys are: {response['TableList']}, {prefix}\"\n",
    "        # raise Exception(error_message)\n",
    "        tables = [table['Name'] for table in response['TableList']]\n",
    "\n",
    "        # Assuming the first table name matches the prefix, use it for reading data\n",
    "        table_name = tables[0]\n",
    "        print(\"crawler_metadata\")\n",
    "\n",
    "\n",
    "\n",
    "        # Read data from Glue Data Catalog\n",
    "        dynamic_frame = glueContext.create_dynamic_frame.from_catalog(\n",
    "            database=database_name,\n",
    "            table_name=table_name\n",
    "        )\n",
    "\n",
    "        # Convert to a Spark DataFrame (or Pandas DataFrame if needed)\n",
    "        data = dynamic_frame.toDF()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(data_frame, environment, target_directory, target_bucket):\n",
    "    # Determine the output path based on the environment\n",
    "    if environment == 'local':\n",
    "\n",
    "        output_path = os.path.join(target_directory, 'output.csv')\n",
    "        print(output_path)\n",
    "        data_frame.write.csv(output_path, mode='overwrite', header=True)\n",
    "    else:\n",
    "        output_path = f\"s3://{target_bucket}/{target_directory}\"\n",
    "        data_frame.write.csv(output_path, mode='overwrite', header=True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+-------------+\n",
      "|First Name|Last Name|               Email|Year of Birth|\n",
      "+----------+---------+--------------------+-------------+\n",
      "|     Rohan|    Gupta|rohan.gupta@wedne...|         1992|\n",
      "+----------+---------+--------------------+-------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "environment = get_env_or_args(\"ENVIRONMENT_NAME\")\n",
    "load_env(environment)\n",
    "local_file_path = f\"{RELATIVE_PATH_PREFIX}/data/raw/sample.csv\"\n",
    "crawler_name = get_env_or_args(\"SOURCE_CRAWLER\")\n",
    "data = read_data(get_env_or_args(\"ENVIRONMENT_NAME\"), local_file_path, crawler_name)\n",
    "print(data.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+-------------+-----------+----+\n",
      "|first_name|last_name|               email|year_of_birth|  full_name| age|\n",
      "+----------+---------+--------------------+-------------+-----------+----+\n",
      "|     Rohan|    Gupta|rohan.gupta@wedne...|         1992|Rohan Gupta|31.0|\n",
      "+----------+---------+--------------------+-------------+-----------+----+\n",
      "\n",
      "None\n",
      "./../../../landing/job2/output.csv\n"
     ]
    }
   ],
   "source": [
    "target_bucket = get_env_or_args(\"BUCKET_NAME\")\n",
    "target_directory = f\"{RELATIVE_PATH_PREFIX if environment == 'local' else target_bucket}/landing/{get_env_or_args('JOB_NAME')}\"\n",
    "transformed_data = transform_to_lower_case(data)\n",
    "final_data_with_age = calculate_age(transformed_data)\n",
    "print(final_data_with_age.show())\n",
    "write_to_csv(final_data_with_age, environment, target_directory, target_bucket)\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
