name: build-and-deploy

# Controls when the action will run. Triggers the workflow on push 
# but only for the master branch.
on:
  push:
    branches:
      - main

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains two jobs called "build" and "deploy"
  build-and-deploy:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v2
        
      # Set up Python
      - name: Set up Python 3.9
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'
          
      # Install nbconvert to convert notebook file to python script
      - name: Install nbconvert
        run: |
          python -m pip install --upgrade pip
          pip install nbconvert
          sudo apt install tree

      # Convert notebook file to python
      - name: Convert notebook
        run: ./scripts/convert-notebooks-to-scripts.sh

      # Get the bucket name from the config/properties.yml
      - name: Read bucket_name from properties.yml
        run: echo "BUCKET_NAME=$(yq e '.bucket_name' config/properties.yml)" >> $GITHUB_ENV
      
      # Get the region from the config/properties.yml
      - name: Read region from properties.yml
        run: echo "REGION=$(yq e '.region' config/properties.yml)" >> $GITHUB_ENV

      # Get the stack_name from the config/properties.yml
      - name: Read region from properties.yml
        run: echo "STACK_NAME=$(yq e '.stack_name' config/properties.yml)" >> $GITHUB_ENV

      - name: Upload to S3
        uses: jakejarvis/s3-sync-action@master
        with:
          args: >
            --exclude *.ipynb 
            --exclude *.json
            --delete
        env:
          AWS_S3_BUCKET: ${{ env.BUCKET_NAME }}
          AWS_REGION: ${{ env.REGION }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          SOURCE_DIR: './src/'
          DEST_DIR: 'scripts'
          with:
          args: >
            --include *.py
            --exclue *.ipynb
            --exclue .env
            --delete

      - name: Upload to S3
        uses: jakejarvis/s3-sync-action@master
        with:
          args: >
            --delete
        env:
          AWS_S3_BUCKET: ${{ secrets.AWS_BUCKET }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          SOURCE_DIR: './data/raw'
          DEST_DIR: 'source'

      # set up AWS Credentials so that we can update the glue job
      - name: Set up AWS credentials
        shell: bash
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ env.REGION }}
          AWS_BUCKET: ${{ env.BUCKET_NAME }}
        run: |
          echo "AWS credentials set up"

      # Update the Glue job to use the new script
      - name: Update Glue job
        run: make update-infra --name=${{ env.STACK_NAME }} --region=${{ env.REGION }}
      
      - name: Commit Changes
        uses: EndBug/add-and-commit@v7
        with:
          author_name: Gitflow
          author_email: git@wednesday.is
          message: 'Updated glue job properties [skip actions]'
          add: '.'
          push: false

      - name: Git pull origin
        run: |
          git pull origin ${{ github.ref }}


      - name: Get branch name
        id: vars
        run: echo ::set-output name=branch_name::${GITHUB_REF#refs/*/}

      - name: Pushing to a protected branch
        uses: CasperWA/push-protected@v2
        with:
          token: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
          branch: ${{ steps.vars.outputs.branch_name }}
          unprotect_reviews: true